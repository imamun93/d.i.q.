{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Can you explain the Bias-Variance Tradeoff?\n",
    "\n",
    "Hint: Think about this in the context of learning algorithms and training data.\n",
    "\n",
    "Can you share an example where/why you would want to use a biased estimator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is when the data being analyzed are all clustered together. This leads to overfitting on the training set of the data and will not predict or categorize the proper outcome when testing on unseen data. In this case, it will overcorrect.\n",
    "\n",
    "Variance is the opposite when all data variables are separated and it appears that there are no relations between each features. This can lead to underfitting of the training algorithms. It will fail to find any relative relationship and will affect multicolieanrity in a negative manner.\n",
    "\n",
    "\n",
    "It is almost always a good idea to use a biased estimator. A bias estimator is calculated through expected value- actual value. In a chemical test, this is similar to expecting a 100% yield but receiving 60% yield of product. Similarly in algorithms, in the training set we see 99% accuracy but in the testing set we receive 60% accuracy. We know that there are some overfitting that occured and can tune the hyperparameters of the features accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
